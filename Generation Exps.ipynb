{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/octavia/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/octavia/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import codecs\n",
    "import string\n",
    "import copy as cp\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn.feature_selection as fs\n",
    "from nltk.corpus import stopwords as st\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.cross_validation import KFold, LeaveOneOut, StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import clone\n",
    "# from ranker import create_occ_matrix, create_rank_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyp = pyphen.Pyphen(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lan-guage'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp.inserted(\"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stwords = st.words('english')\n",
    "\n",
    "def get_preprocessor(suffix=''):\n",
    "    def preprocess(unicode_text):\n",
    "        return unicode(unicode_text.strip().lower() + suffix)\n",
    "    return preprocess\n",
    "\n",
    "\n",
    "def preprocess_data(X, n, suffix='', binarize=True):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1),\n",
    "                                 preprocessor=get_preprocessor(suffix))\n",
    "    X = vectorizer.fit_transform(X)\n",
    "    X = Binarizer(copy=False).fit_transform(X) if binarize else X\n",
    "    return X\n",
    "\n",
    "\n",
    "def preprocess_lyric(lyric):\n",
    "    new_lyric = cp.deepcopy(lyric)\n",
    "#     pct = ((string.punctuation).replace(\"_\", \"\")).replace(\"'\", \"\")    \n",
    "    pct = (string.punctuation).replace(\"_\", \"\")\n",
    "    new_lyric = new_lyric.translate(str.maketrans('','', pct))\n",
    "#     new_lyric = new_lyric.decode(\"utf8\")\n",
    "    new_lyric = new_lyric.replace(\"_\", \" _ \")\n",
    "    new_lyric = new_lyric.replace(\"'\", \" \")\n",
    "    new_lyric = new_lyric.lower()\n",
    "    new_lyric = new_lyric.split()\n",
    "    return new_lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename='sm-vs-all-lyrics.txt'):\n",
    "    lyrics, y = [], []\n",
    "\n",
    "    with codecs.open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            aux = line.split(\"\\t\")\n",
    "            if len(aux) != 2:\n",
    "                print(\"aux\", aux)\n",
    "            else:\n",
    "                lyr, label = aux[0], aux[1]\n",
    "                lyrics.append(preprocess_lyric(lyr))\n",
    "                y.append(int(label))\n",
    "\n",
    "    lyrics, y = np.array(lyrics), np.array(y, dtype=np.int)\n",
    "    return lyrics, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_verses(x):\n",
    "    new_x = []\n",
    "    for i, lyric in enumerate(x):\n",
    "        new_x.append((\" \".join(x[i])) + \" $tay\") # append the entire lyrics\n",
    "    return new_x    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (\" \".join(x[0])), x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_x = merge_verses(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'david the wind blows _ the wind blows _ bits of your life away _ your friends all say _ where is our boy oh weve lost our boy _ but they should know _ where youve gone _ because again and again youve explained that _ youre going to _ _ oh youre going to _ yeah yeah yeah yeah _ england for the english _ england for the english _ _ david the winds blow _ the winds blow _ all of my dreams away _ and i still say _ where is our boy _ ah weve lost our boy _ but i should know _ why youve gone _ because again and again youve explained _ youve gone to the _ _ national ah _ to the national _ theres a country you dont live there _ but one day you would like to _ and if you show them what youre made of _ oh then you might do _ _ but david we wonder _ we wonder if the thunder _ is ever really gonna begin _ begin begin _ your mom says _ ive lost my boy _ but she should know _ why youve gone _ because again and again youve explained _ youve gone to the _ _ national _ to the national _ to the national front disco _ because you want the day to come sooner _ you want the day to come sooner _ you want the day to come sooner _ when youve settled the score _ _ oh the national repeat x5 $tay'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"smiths_merged.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for lyric in new_x:\n",
    "        f.write(lyric + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/octavia/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textgen = textgenrnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,407 texts collected.\n",
      "Training on 2,360,441 character sequences.\n",
      "Epoch 1/1\n",
      "18440/18440 [==============================] - 4023s 218ms/step - loss: 1.3344\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "dededmdeeded oh how i say _ i can see the stars _ the light in the stars _ and the stars of the stars _ and i know its all on the love _ i want to be the one _ and i can see the control _ and its a floor _ _ i want to be the world _ when i dont know it was in the one _ i am a body so longer _ _ i \n",
      "\n",
      "edddeded sun _ _ and i want to be all _ _ i want to say _ _ i can see it _ she says i love you _ _ i want to be a longer of a long _ and i dont want to say _ i want to be a longer _ i dont care i could be a longer _ _ i want to be a longer _ and i can see the stars _ _ and i want to be a longer _ \n",
      "\n",
      "meedmede _ and its all the love _ i cant see the stars _ and i can see the stars _ and i cant wait to be the world _ its not a line _ _ i want to be a longer _ and i want to be a world _ i want to be a longer _ _ i dont want to be a longer _ i cant see the world _ _ i want to be a longer _ and i c\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "dsdeeid i love really two _ i dont care _ im all on the world _ and when the days that long flood _ she will never drowned _ _ _ you should i need you _ _ _ now its all on the colour the clots of home _ _ _ i looked a litter of a love _ _ i ever have to be already and and i fell to have a distore \n",
      "\n",
      "ddldmemeeey for a promise _ on the sprinit on the sun _ _ and i want to take it _ i dont want to change you _ _ _ than the sound _ the mind of the cold sense of love _ i love you _ a new eyes and im a colours _ and im so stranger _ _ and i can are all the books on the boys _ _ but the cold your fa\n",
      "\n",
      "ameemds _ can i have to be a stranger _ the bed would go _ but i can see you to strange _ i love you _ _ and its always world _ to try to the beauty _ youve loved its longher _ i want to want to starce _ _ i dont care _ and i dont love me _ _ i cant have to be stand of the drowning _ and i love yo\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "sayamnsors now im stranger would _ you are put _ i want for you _ youd have a breads angel in a rounds holiday _ bock out of clearings cutilfly ras a come _ done she isnticals happening changes lay _ drooms so lonely descurd road _ oh do true and tomorron sand _ itll ugb in fridire together _ and \n",
      "\n",
      "yyy is its no turning from your thought _ me words _ of a hand stand a guilt of a go _ more as the curiss in a dim in sometimes with traul _ i grow decidispious bone _ that you keep it you cant sign away _ _ teller my heart _ and onlybody should pretoctine and i thing _ my happy stard _ today me l\n",
      "\n",
      "mmmdaehdshmdieed the stars in a red face from the bye longo _ an done advob and hay _ ener _ longeers wand me last much in the boys _ _ _ its matter charryonera drams _ for enh be a next of idu _ apoarly to kneel _ im uniods before all out _ the sait weetles begin _ got the kill streetfable chants\n",
      "\n",
      "deitlemmdey the without take _ _ i wanted to try _ and i want to see what i love _ _ the things i feel to hear my heart _ _ and its not one for me _ and i would only was your mouth _ _ and i cant have to be round _ i have to be the lovers a long _ single of my heart _ done and change said the colo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.train_from_file('smiths_merged.txt', num_epochs=1)\n",
    "textgen.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dmeeaeeled deep secret _ and i wanna say im alone _ _ its not gone _ _ no one of me _ _ to him _ i can take me to the tops _ what has no longer _ no more lie _ _ i think im without me _ the first to the look about the room _ its need to end _ _ i want to go _ hey i cant look at my princes _ and i \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(textgen.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(textgen.generate(3, temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = 0\n",
    "for i in range(len(y)):\n",
    "    if(y[i] == 1 ):\n",
    "        ct += 1\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb = []\n",
    "for word in x[0]:\n",
    "    if word != \"_\":\n",
    "        bb.append(str(len(hyp.inserted(word).split(\"-\"))))\n",
    "    else:\n",
    "        bb.append(\"_\")\n",
    "        \n",
    "b = (\" \".join(bb)).split(\"_\")\n",
    "# b\n",
    "# len(x[0]) == len(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyphenate(lyric):\n",
    "    '''hyphenate lyrics of a song'''\n",
    "    hyp_lyric = []\n",
    "    for word in lyric:\n",
    "        if word != \"_\":\n",
    "            hyp_lyric.append(hyp.inserted(word).split(\"-\"))\n",
    "        else:\n",
    "            hyp_lyric.append(word)\n",
    "#     print(\"hyp_lyric\", hyp_lyric)\n",
    "    return hyp_lyric\n",
    "\n",
    "def count_syllables(lyric):\n",
    "    syl_lyric, pat_lyric = [], []\n",
    "#     print(\"lyr\", lyric)\n",
    "    for word in lyric:\n",
    "        if word != \"_\":\n",
    "            syl_lyric.append(str(len(word))) # split word into syllables and count\n",
    "        else:\n",
    "            syl_lyric.append(word)\n",
    "    syl_lyric = (\" \".join(syl_lyric)) # unite word syllable counts into verses\n",
    "    print(\"syl\", syl_lyric)\n",
    "    \n",
    "    for i in range(len(syl_lyric)): # for each verse\n",
    "        if syl_lyric[i] != \"_\":\n",
    "            pat_lyric.append(str(sum([int(c) for c in syl_lyric[i].split()])))\n",
    "        else:\n",
    "            pat_lyric.append(\"_\")\n",
    "    pat_lyric = (\" \".join(pat_lyric)).split(\"_\")\n",
    "    \n",
    "    print (\"syl, pat\", syl_lyric, pat_lyric)\n",
    "    return syl_lyric, pat_lyric\n",
    "\n",
    "\n",
    "def get_verse_patterns(lyrics):\n",
    "    \n",
    "    hyp_lyrics, syl_lyrics = [], []\n",
    "    for lyric in lyrics:\n",
    "        hyp_lyric = hyphenate(lyric) \n",
    "        hyp_lyrics.append(hyp_lyric)  # save\n",
    "        \n",
    "        syl_lyric = count_syllables(hyp_lyric)\n",
    "        syl_lyrics.append(syl_lyric)  # save\n",
    "             \n",
    "    print(len(hyp_lyrics) == len(lyrics), len(syl_lyrics) == len(lyrics))\n",
    "\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syl 1 1 1 1 _ 1 1 1 _ 1 1 1 1 1 _ 1 1 1 1 _ 1 1 1 1 1 1 1 1 1 _ 1 1 1 1 _ 1 2 1 _ 2 1 1 1 2 2 1 _ 1 2 1 _ _ 1 1 2 1 _ 1 1 1 1 _ 2 1 1 2 _ 2 1 1 2 _ _ 1 1 1 1 _ 1 1 1 _ 1 1 1 1 1 _ 1 1 1 1 _ 1 1 1 1 _ 1 1 1 1 1 _ 1 1 1 1 _ 1 2 1 _ 2 1 1 1 2 2 _ 2 1 1 1 _ _ 3 1 _ 1 1 3 _ 1 1 2 1 1 1 1 _ 1 1 1 1 1 1 1 _ 1 1 1 1 1 1 1 1 1 _ 1 1 1 1 1 _ _ 1 1 1 2 _ 1 2 1 1 2 _ 1 2 3 1 2 _ 2 2 _ 1 1 1 _ 1 1 1 1 _ 1 1 1 1 _ 1 2 1 _ 2 1 1 1 2 2 _ 2 1 1 1 _ _ 3 _ 1 1 3 _ 1 1 3 1 2 _ 2 1 1 1 1 1 1 2 _ 1 1 1 1 1 1 2 _ 1 1 1 1 1 1 2 _ 1 2 2 1 1 _ _ 1 1 3 2 1\n",
      "syl, pat 1 1 1 1 _ 1 1 1 _ 1 1 1 1 1 _ 1 1 1 1 _ 1 1 1 1 1 1 1 1 1 _ 1 1 1 1 _ 1 2 1 _ 2 1 1 1 2 2 1 _ 1 2 1 _ _ 1 1 2 1 _ 1 1 1 1 _ 2 1 1 2 _ 2 1 1 2 _ _ 1 1 1 1 _ 1 1 1 _ 1 1 1 1 1 _ 1 1 1 1 _ 1 1 1 1 _ 1 1 1 1 1 _ 1 1 1 1 _ 1 2 1 _ 2 1 1 1 2 2 _ 2 1 1 1 _ _ 3 1 _ 1 1 3 _ 1 1 2 1 1 1 1 _ 1 1 1 1 1 1 1 _ 1 1 1 1 1 1 1 1 1 _ 1 1 1 1 1 _ _ 1 1 1 2 _ 1 2 1 1 2 _ 1 2 3 1 2 _ 2 2 _ 1 1 1 _ 1 1 1 1 _ 1 1 1 1 _ 1 2 1 _ 2 1 1 1 2 2 _ 2 1 1 1 _ _ 3 _ 1 1 3 _ 1 1 3 1 2 _ 2 1 1 1 1 1 1 2 _ 1 1 1 1 1 1 2 _ 1 1 1 1 1 1 2 _ 1 2 2 1 1 _ _ 1 1 3 2 1 ['1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 2 0 1 0 ', ' 0 2 0 1 0 1 0 1 0 2 0 2 0 1 0 ', ' 0 1 0 2 0 1 0 ', ' 0 ', ' 0 1 0 1 0 2 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 ', ' 0 2 0 1 0 1 0 2 0 ', ' 0 2 0 1 0 1 0 2 0 ', ' 0 ', ' 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 2 0 1 0 ', ' 0 2 0 1 0 1 0 1 0 2 0 2 0 ', ' 0 2 0 1 0 1 0 1 0 ', ' 0 ', ' 0 3 0 1 0 ', ' 0 1 0 1 0 3 0 ', ' 0 1 0 1 0 2 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 1 0 ', ' 0 ', ' 0 1 0 1 0 1 0 2 0 ', ' 0 1 0 2 0 1 0 1 0 2 0 ', ' 0 1 0 2 0 3 0 1 0 2 0 ', ' 0 2 0 2 0 ', ' 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 1 0 1 0 1 0 ', ' 0 1 0 2 0 1 0 ', ' 0 2 0 1 0 1 0 1 0 2 0 2 0 ', ' 0 2 0 1 0 1 0 1 0 ', ' 0 ', ' 0 3 0 ', ' 0 1 0 1 0 3 0 ', ' 0 1 0 1 0 3 0 1 0 2 0 ', ' 0 2 0 1 0 1 0 1 0 1 0 1 0 1 0 2 0 ', ' 0 1 0 1 0 1 0 1 0 1 0 1 0 2 0 ', ' 0 1 0 1 0 1 0 1 0 1 0 1 0 2 0 ', ' 0 1 0 2 0 2 0 1 0 1 0 ', ' 0 ', ' 0 1 0 1 0 3 0 2 0 1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1 1 1 1 _ 1 1 1 _ 1 1 1 1 1 _ 1 1 1 1 _ 1 1 1 1 1 1 1 1 1 _ 1 1 1 1 _ 1 2 1 _ 2 1 1 1 2 2 1 _ 1 2 1 _ _ 1 1 2 1 _ 1 1 1 1 _ 2 1 1 2 _ 2 1 1 2 _ _ 1 1 1 1 _ 1 1 1 _ 1 1 1 1 1 _ 1 1 1 1 _ 1 1 1 1 _ 1 1 1 1 1 _ 1 1 1 1 _ 1 2 1 _ 2 1 1 1 2 2 _ 2 1 1 1 _ _ 3 1 _ 1 1 3 _ 1 1 2 1 1 1 1 _ 1 1 1 1 1 1 1 _ 1 1 1 1 1 1 1 1 1 _ 1 1 1 1 1 _ _ 1 1 1 2 _ 1 2 1 1 2 _ 1 2 3 1 2 _ 2 2 _ 1 1 1 _ 1 1 1 1 _ 1 1 1 1 _ 1 2 1 _ 2 1 1 1 2 2 _ 2 1 1 1 _ _ 3 _ 1 1 3 _ 1 1 3 1 2 _ 2 1 1 1 1 1 1 2 _ 1 1 1 1 1 1 2 _ 1 1 1 1 1 1 2 _ 1 2 2 1 1 _ _ 1 1 3 2 1',\n",
       " ['1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 2 0 1 0 ',\n",
       "  ' 0 2 0 1 0 1 0 1 0 2 0 2 0 1 0 ',\n",
       "  ' 0 1 0 2 0 1 0 ',\n",
       "  ' 0 ',\n",
       "  ' 0 1 0 1 0 2 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 2 0 1 0 1 0 2 0 ',\n",
       "  ' 0 2 0 1 0 1 0 2 0 ',\n",
       "  ' 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 2 0 1 0 ',\n",
       "  ' 0 2 0 1 0 1 0 1 0 2 0 2 0 ',\n",
       "  ' 0 2 0 1 0 1 0 1 0 ',\n",
       "  ' 0 ',\n",
       "  ' 0 3 0 1 0 ',\n",
       "  ' 0 1 0 1 0 3 0 ',\n",
       "  ' 0 1 0 1 0 2 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 ',\n",
       "  ' 0 1 0 1 0 1 0 2 0 ',\n",
       "  ' 0 1 0 2 0 1 0 1 0 2 0 ',\n",
       "  ' 0 1 0 2 0 3 0 1 0 2 0 ',\n",
       "  ' 0 2 0 2 0 ',\n",
       "  ' 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 ',\n",
       "  ' 0 1 0 2 0 1 0 ',\n",
       "  ' 0 2 0 1 0 1 0 1 0 2 0 2 0 ',\n",
       "  ' 0 2 0 1 0 1 0 1 0 ',\n",
       "  ' 0 ',\n",
       "  ' 0 3 0 ',\n",
       "  ' 0 1 0 1 0 3 0 ',\n",
       "  ' 0 1 0 1 0 3 0 1 0 2 0 ',\n",
       "  ' 0 2 0 1 0 1 0 1 0 1 0 1 0 1 0 2 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 1 0 1 0 2 0 ',\n",
       "  ' 0 1 0 1 0 1 0 1 0 1 0 1 0 2 0 ',\n",
       "  ' 0 1 0 2 0 2 0 1 0 1 0 ',\n",
       "  ' 0 ',\n",
       "  ' 0 1 0 1 0 3 0 2 0 1'])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(b[0], sum([int(c) for c in b[0].split()]))\n",
    "xx = hyphenate(x[0])\n",
    "count_syllables(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
